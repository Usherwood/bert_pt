{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ULM Preprocessing: BPE\n",
    "\n",
    "This file is to preprocess and text file for use in the ULM, note this notebook is for the full emoji BPE implamentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import pickle\n",
    "import pathlib\n",
    "import os\n",
    "import pathlib\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "import pathlib\n",
    "import os\n",
    "import progressbar\n",
    "\n",
    "from text_preprocessing import tokenizer_word\n",
    "from language_model_processing import read_raw_data_preprocess_and_save, create_vocab_df\n",
    "from bpe import create_token_vocabulary, get_stats, merge_vocab, Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following inputs are required from the user:\n",
    "- The name of the dataset as stored in datasets/\n",
    "- The input type, options include tokens, txt or csv. Note this affects more than just the file type. TODO\n",
    "- DATASET_FILE_MAP which dictates if the data is pre-split into train/val/test, if so we will keep the datasets like this, if not we split the master corpus into the subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_NAME = \"wikitext-2\"\n",
    "INPUT_TYPE = 'tokens' #Options: tokens, txt, csv\n",
    "TO_SPLIT_CLITICS = False #Set to false if clitics already tokenized\n",
    "DATASET_FILE_MAP = {'all': None,\n",
    "                    'train': 'wiki.train.tokens', 'validate': 'wiki.valid.tokens', 'test': 'wiki.test.tokens'}\n",
    "\n",
    "if DATASET_FILE_MAP.get('all'):\n",
    "    file_split = 'all'\n",
    "else:\n",
    "    file_split = 'split'\n",
    "\n",
    "UNK_TOKEN = \"_unk_\" #none if isnt one\n",
    "SEQUENCE_LENGTH = 20\n",
    "NUM_MERGES = 50 #VOCABULARY_SIZE = NUM_MERGES + N_BYTES (~1500)\n",
    "\n",
    "mini_batch_size = 64\n",
    "N_SPLITS = 10 #of training data on disk, this is purely a question of resource efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproccesing\n",
    "Here we split the raw data into a set of small csvs with the data tokenized to the form that we need. If there is a pre-definition on train/test/val we will put them in the corresponding folders here, if not at the preprocessing stage everything will be dumped into asingle \"all folder\".\n",
    "\n",
    "The tokenization occuring here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tokens = []\n",
    "zx = 0\n",
    "\n",
    "#An attempt to make this platform agnostic\n",
    "notebook_dir = pathlib.Path.cwd()\n",
    "repo_dir = notebook_dir.parent\n",
    "(repo_dir / \"models\").mkdir(exist_ok = True)\n",
    "(repo_dir / \"models\" / \"ULM\").mkdir(exist_ok = True)\n",
    "dataset_dir = repo_dir / \"datasets\" / \"ULM\" / DATASET_NAME\n",
    "models_dir = repo_dir / \"models\" / \"ULM\"\n",
    "(models_dir / DATASET_NAME).mkdir(exist_ok = True)\n",
    "(models_dir / DATASET_NAME / \"preprocessed_ulm_data\").mkdir(exist_ok = True)\n",
    "(models_dir / DATASET_NAME / \"language_maps\").mkdir(exist_ok = True)\n",
    "    \n",
    "models_dir = models_dir / DATASET_NAME\n",
    "    \n",
    "read_raw_data_preprocess_and_save(dataset_file_map=DATASET_FILE_MAP, \n",
    "                                  models_dir=models_dir, \n",
    "                                  dataset_dir=dataset_dir,\n",
    "                                  input_type=INPUT_TYPE,\n",
    "                                  split_clitics=TO_SPLIT_CLITICS,\n",
    "                                  remove_numbers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "notebook_dir = pathlib.Path.cwd()\n",
    "repo_dir = notebook_dir.parent\n",
    "\n",
    "if file_split == 'all':\n",
    "    preprocessed_training_data_dir = repo_dir / \"models\" / \"ULM\" / DATASET_NAME / \"preprocessed_ulm_data\"\n",
    "else:\n",
    "    preprocessed_training_data_dir = repo_dir / \"models\" / \"ULM\" / DATASET_NAME / \"preprocessed_ulm_data\" / \"train\"\n",
    "\n",
    "for file in os.listdir(preprocessed_training_data_dir):\n",
    "    with open(os.path.join(preprocessed_training_data_dir, file), 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        words = list(reader)[0]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# U+E000..U+F8FF is defined as a private use area so we use for space and unk\n",
    "unk = chr(int(\"E000\", 16))\n",
    "spc = chr(int(\"E001\", 16))\n",
    "\n",
    "language_maps_dir = repo_dir / \"models\" / \"ULM\" / DATASET_NAME / \"language_maps\"\n",
    "(language_maps_dir).mkdir(exist_ok = True)\n",
    "\n",
    "def save_obj(obj, directory, name):\n",
    "    with open(directory / \"{}.pkl\".format(name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "id_to_vocab = create_token_vocabulary()\n",
    "unk_id = len(id_to_vocab)\n",
    "spc_id = len(id_to_vocab) + 1\n",
    "\n",
    "id_to_vocab[unk_id] = unk\n",
    "id_to_vocab[spc_id] = spc\n",
    "\n",
    "save_obj(id_to_vocab, language_maps_dir, \"byte_decoder\")\n",
    "\n",
    "vocab_to_id = {v: i for i, v in id_to_vocab.items()}\n",
    "id_to_vocab = {i: v for v, i in vocab_to_id.items()}  # Reverse as the emoji and other characters have some overlap \n",
    "_ = vocab_to_id.pop(unk)\n",
    "\n",
    "print(\"BPE vocab size:\", len(vocab_to_id))\n",
    "\n",
    "corpus = tokenizer_word(corpus,\n",
    "                        keep_phrases=False,\n",
    "                        tokenize_punc=True,\n",
    "                        split_clitics=True,\n",
    "                        keep_preceeding_space=True)\n",
    "\n",
    "corpus_ids = [[vocab_to_id.get(l, unk_id) if l is not \" \" else spc_id for l in word] for word in corpus]\n",
    "corpus = [\" \".join([id_to_vocab[l] for l in word]) for word in corpus_ids]\n",
    "\n",
    "count_dict = Counter(corpus)\n",
    "vocab_df = pd.DataFrame(np.array([list(dict(count_dict).keys()), list(dict(count_dict).values())]).T,\n",
    "                        columns=['Word', 'Freq'])\n",
    "vocab_df['Freq'] = vocab_df['Freq'].astype(np.float64)\n",
    "print(\"Total word vocab size\", len(vocab_df))\n",
    "df_dict = vocab_df.sort_values(by=['Freq'], ascending=False).to_dict(\"records\")\n",
    "df_dict = {item[\"Word\"]: item[\"Freq\"] for item in df_dict}\n",
    "\n",
    "bpe_merges = []\n",
    "vocab_to_id_current_max_id = sorted(list(vocab_to_id.values()))[-1]\n",
    "with progressbar.ProgressBar(max_value=NUM_MERGES) as bar:\n",
    "    for i in range(NUM_MERGES):\n",
    "        vocab_to_id_current_max_id += 1\n",
    "        pairs = get_stats(df_dict)\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        bpe_merges.append(best)\n",
    "        vocab_to_id[\"\".join(best)] = vocab_to_id_current_max_id\n",
    "        df_dict = merge_vocab(best, df_dict)\n",
    "        bar.update(i)\n",
    "id_to_vocab = {i: v for v, i in vocab_to_id.items()}\n",
    "id_to_vocab[unk_id] = unk\n",
    "\n",
    "save_obj(bpe_merges, language_maps_dir, \"bpe_merges\")\n",
    "save_obj(id_to_vocab, language_maps_dir, \"id_to_vocab\")\n",
    "save_obj(vocab_to_id, language_maps_dir, \"vocab_to_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_maps_dir = repo_dir / \"models\" / \"ULM\" / DATASET_NAME / \"language_maps\"\n",
    "processed_data_dir = language_maps_dir.parent / \"processed_ulm_data\"\n",
    "processed_data_dir.mkdir(exist_ok = True)\n",
    "training_data_dir = processed_data_dir / \"train\"\n",
    "test_data_dir = processed_data_dir / \"test\"\n",
    "validation_data_dir = processed_data_dir / \"validate\"\n",
    "training_data_dir.mkdir(exist_ok = True)\n",
    "test_data_dir.mkdir(exist_ok = True)\n",
    "validation_data_dir.mkdir(exist_ok = True)\n",
    "\n",
    "def load_obj(name, directory):\n",
    "    with open(os.path.join(directory, name + '.pkl'), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "byte_decoder = load_obj(\"byte_decoder\", language_maps_dir)\n",
    "id_to_vocab = load_obj(\"id_to_vocab\", language_maps_dir)\n",
    "vocab_to_id = load_obj(\"vocab_to_id\", language_maps_dir)\n",
    "bpe_merges = load_obj(\"bpe_merges\", language_maps_dir)\n",
    "\n",
    "VOCABULARY_SIZE = len(id_to_vocab)\n",
    "print(\"VOCABULARY_SIZE:\", VOCABULARY_SIZE)\n",
    "\n",
    "(training_data_dir / 'X').mkdir(exist_ok = True)\n",
    "(test_data_dir / 'X').mkdir(exist_ok = True)\n",
    "(validation_data_dir / 'X').mkdir(exist_ok = True)\n",
    "(training_data_dir / 'Y').mkdir(exist_ok = True)\n",
    "(test_data_dir / 'Y').mkdir(exist_ok = True)\n",
    "(validation_data_dir / 'Y').mkdir(exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexify and Split Data\n",
    "\n",
    "The sections below create the samples (sequences + the next word), then shuffle them randomly, if file_split='all' then split into train, test and validate groupings. These are then saved into files (split across N_SPLITS files) in the relevant directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_samples(data, sequence_length, stride_length=1):\n",
    "    return [(data[x:x+sequence_length], data[x+sequence_length]) for x in range(0, len(data)-sequence_length, stride_length)]\n",
    "\n",
    "def save_split_object(obj, directory, name, splits, mini_batch_size):\n",
    "    \n",
    "    actual_split_sizes = []\n",
    "    \n",
    "    if(len(obj)<splits):\n",
    "        raise ValueError(\"Too few items for number of splits\")\n",
    "    ideal_total_per_split = math.floor(len(obj)/splits)\n",
    "    remainder = 0\n",
    "    for i in range(splits):\n",
    "        split_total = math.floor((ideal_total_per_split+remainder)/mini_batch_size) * mini_batch_size\n",
    "        if (i+1)*split_total > len(obj):\n",
    "            split_total -= mini_batch_size\n",
    "        remainder = ideal_total_per_split - split_total\n",
    "        lower = i*split_total\n",
    "        upper = (i+1)*split_total\n",
    "        save_obj(obj[lower:upper], directory, \"{}_{}\".format(name,i))\n",
    "        actual_split_sizes += [split_total]\n",
    "        \n",
    "    save_obj(actual_split_sizes, directory.parent, name+'_batch_sizes')\n",
    "    \n",
    "encoder = Encoder(\n",
    "                  encoder=vocab_to_id,\n",
    "                  bpe_merges=bpe_merges,\n",
    "                  byte_decoder=byte_decoder,\n",
    "                  unk_token=chr(int(\"E000\", 16)),\n",
    "                  unk_id=len(byte_decoder)-2,\n",
    "                  spc_token=chr(int(\"E001\", 16)),\n",
    "                  spc_id=len(byte_decoder)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = encoder.encode(\"Hi, i am a data scientist and I love emoji 😎🤢 我\")\n",
    "print(x)\n",
    "for a in x:\n",
    "    print(encoder.decode([a]))\n",
    "print(encoder.decode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_key in DATASET_FILE_MAP.keys():\n",
    "    \n",
    "    if file_split == 'all':\n",
    "        preprocessed_data_dir = models_dir / \"preprocessed_ulm_data\"\n",
    "    else:\n",
    "        preprocessed_data_dir = models_dir / \"preprocessed_ulm_data\" / dataset_key\n",
    "    \n",
    "    words = []\n",
    "    for file in os.listdir(preprocessed_data_dir):\n",
    "        with open(os.path.join(preprocessed_data_dir, file), 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            words += list(reader)[0]\n",
    "    print('Total number of words:', len(words))\n",
    "\n",
    "    data = encoder.encode(\" \".join(words))\n",
    "\n",
    "    #Shuffle and split data into train, test, validate\n",
    "    zipped_samples = create_samples(data, SEQUENCE_LENGTH)\n",
    "    np.random.shuffle(zipped_samples)\n",
    "\n",
    "    train_size = 0.7\n",
    "    test_size = 0.2\n",
    "    #validate_size is balance\n",
    "\n",
    "    if file_split == 'all':\n",
    "        total_samples = len(zipped_samples)\n",
    "        train_cutoff = math.floor(total_samples*train_size)\n",
    "        test_cutoff = math.floor(total_samples*test_size)\n",
    "        X_train, Y_train = zip(*zipped_samples[0:train_cutoff])\n",
    "        X_test, Y_test = zip(*zipped_samples[train_cutoff:train_cutoff + test_cutoff])\n",
    "        X_validate, Y_validate = zip(*zipped_samples[train_cutoff + test_cutoff:])\n",
    "\n",
    "        #test case\n",
    "        testing_index = np.random.randint(0, len(X_train))\n",
    "        print(X_train[testing_index],\n",
    "              Y_train[testing_index],\n",
    "              [id_to_vocab[tok] for tok in X_train[testing_index]],\n",
    "              id_to_vocab[Y_train[testing_index]])\n",
    "\n",
    "        #Save data, split up into chunks\n",
    "        save_split_object(X_train, training_data_dir / 'X', 'X', N_SPLITS, mini_batch_size)\n",
    "        save_split_object(Y_train, training_data_dir / 'Y', 'Y', N_SPLITS, mini_batch_size)\n",
    "\n",
    "        save_split_object(X_test, test_data_dir / 'X', 'X', N_SPLITS, mini_batch_size)\n",
    "        save_split_object(Y_test, test_data_dir / 'Y', 'Y', N_SPLITS, mini_batch_size)\n",
    "\n",
    "        save_split_object(X_validate, validation_data_dir / 'X', 'X', N_SPLITS, mini_batch_size)\n",
    "        save_split_object(Y_validate, validation_data_dir / 'Y', 'Y', N_SPLITS, mini_batch_size)\n",
    "        \n",
    "    else:\n",
    "        X, Y= zip(*zipped_samples)\n",
    "        \n",
    "        #test case\n",
    "        testing_index = np.random.randint(0, len(X))\n",
    "        print(X[testing_index],\n",
    "              Y[testing_index],\n",
    "              [id_to_vocab[tok] for tok in X[testing_index]],\n",
    "              id_to_vocab[Y[testing_index]])\n",
    "\n",
    "        #Save data, split up into chunks\n",
    "        save_split_object(X, processed_data_dir / dataset_key / 'X', 'X', N_SPLITS, mini_batch_size)\n",
    "        save_split_object(Y, processed_data_dir / dataset_key / 'Y', 'Y', N_SPLITS, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
